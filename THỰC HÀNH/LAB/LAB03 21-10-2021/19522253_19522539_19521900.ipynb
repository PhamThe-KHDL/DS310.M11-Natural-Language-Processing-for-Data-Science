{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"19522253_19522539_19521900.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"10bgvHDS70Rt9Oh0-xbo-fxopr6elnzIh","authorship_tag":"ABX9TyMGl6wd8L0G0Y5Z2xpy8TbR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"W5nL6XRR72P5"},"source":["Họ và tên: Phạm Đức Thể\n","\n","MSSV: 19522253\n","\n","Lớp: DS310.M11\n","\n","Lab 02 - 21/10/2021"]},{"cell_type":"markdown","metadata":{"id":"FKJSq43UX-A5"},"source":["# **Bài tập Thực hành 2 - Huấn luyện mô hình ngôn ngữ dựa trên mạng nơ-ron nhân tạo**\n","\n","**Đề bài**: Huấn luyện một mô hình ngôn ngữ tiếng Việt ở định dạng chữ thường (định dạng lowercase) và dựa trên cấp độ từ (word level). Tập từ vựng chỉ được tính trên tập train, không tính trên tập dev và test. Những từ có tần số xuất hiện nhỏ hơn hoặc bằng 1 được coi là `<unk>`.\n","\n","* **Dữ liệu**: gồm ba tập tin \"VLSP2013_raw_train.txt\", \"VLSP2013_raw_dev.txt\", và \"VLSP2013_raw_test.txt\" `[2]`. Lưu ý, rằng các tập tin này đã được tách từ sẵn và ở định dạng nguyên bản.\n","* **Phương pháp**: Feed-forward Neural Network Language Model `[1]`\n","\n","Sau khi thực hiện xong thí nghiệm, sinh viên sẽ phải trả lời các câu hỏi sau:\n","<font color='yellow'>\n","1. Số câu văn huấn luyện trong tập train, dev và test là bao nhiêu?\n","2. Kích thước của tập từ vựng là bao nhiêu?\n","3. Giá trị Perplexity khi cho mô hình dự đoán trên tập train, dev và test là bao nhiêu?\n","4. Ghi nhận lại 5 câu văn mà mô hình ngôn ngữ cho giá trị log-likelihood cao nhất trên tập test. Phân tích nhanh kết quả thu được.\n","5. Ghi nhận lại 5 câu văn mà mô hình ngôn ngữ cho giá trị log-likelihood thấp nhất trên tập test. Phân tích nhanh kết quả thu được.</font>\n","\n","<font color='red'>Định dạng nộp bài: MSSV1_..._MSSVn.ipynb</font> (tập tin Jupyter Notebook hoặc Google Colab). Khuyến khích nên thực hiện theo nhóm đồ án.\n","\n","Tài liệu tham khảo:\n","\n","`[1]` https://github.com/neubig/anlp-code/blob/main/03-lm/nn-lm.py\n","\n","`[2]` https://vlsp.org.vn/vlsp2013/eval/ws-pos\n"]},{"cell_type":"markdown","metadata":{"id":"TGPW9Z2Gkyye"},"source":["##Advanced NLP Code Examples"]},{"cell_type":"code","metadata":{"id":"TorHKZZFTmrC","executionInfo":{"status":"ok","timestamp":1638865974381,"user_tz":-420,"elapsed":317,"user":{"displayName":"Phạm Đức Thể","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0DbvnWDzLWfReqJkA_CpifKsPoGK79L7Ow3Jg1w=s64","userId":"09005724978234100310"}}},"source":["import math\n","import time\n","import random\n","import os, sys\n","\n","from tqdm import tqdm\n","import torch\n","import torch.nn as nn\n","from torch.autograd import Variable"],"execution_count":99,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BtA7fyoDKYBo"},"source":["### Feed-forward Neural Network Language Model"]},{"cell_type":"code","metadata":{"id":"lFoEO-GgkEWJ","executionInfo":{"status":"ok","timestamp":1638865974744,"user_tz":-420,"elapsed":11,"user":{"displayName":"Phạm Đức Thể","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0DbvnWDzLWfReqJkA_CpifKsPoGK79L7Ow3Jg1w=s64","userId":"09005724978234100310"}}},"source":["class FNN_LM(nn.Module):\n","  def __init__(self, nwords, emb_size, hid_size, num_hist):\n","    super(FNN_LM, self).__init__()\n","    self.embedding = nn.Embedding(nwords, emb_size)\n","    self.fnn = nn.Sequential(\n","      nn.Linear(num_hist*emb_size, hid_size),\n","      nn.Tanh(),\n","      nn.Linear(hid_size, nwords)\n","    )\n","\n","  def forward(self, words):\n","    emb = self.embedding(words)      # 3D Tensor of size [batch_size x num_hist x emb_size]\n","    feat = emb.view(emb.size(0), -1) # 2D Tensor of size [batch_size x (num_hist*emb_size)]\n","    logit = self.fnn(feat)           # 2D Tensor of size [batch_size x nwords]\n","\n","    return logit"],"execution_count":100,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hwm9i1O1KsVw","executionInfo":{"status":"ok","timestamp":1638865974745,"user_tz":-420,"elapsed":11,"user":{"displayName":"Phạm Đức Thể","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0DbvnWDzLWfReqJkA_CpifKsPoGK79L7Ow3Jg1w=s64","userId":"09005724978234100310"}}},"source":["N = 2 # The length of the n-gram\n","EMB_SIZE = 128 # The size of the embedding\n","HID_SIZE = 128 # The size of the hidden layer\n","MAX_LEN = 100\n","\n","USE_CUDA = torch.cuda.is_available()"],"execution_count":101,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6K3qbPOfKucZ"},"source":["### Functions to read in the corpus"]},{"cell_type":"code","metadata":{"id":"vVUAdpOhkEcC","executionInfo":{"status":"ok","timestamp":1638865974746,"user_tz":-420,"elapsed":11,"user":{"displayName":"Phạm Đức Thể","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0DbvnWDzLWfReqJkA_CpifKsPoGK79L7Ow3Jg1w=s64","userId":"09005724978234100310"}}},"source":["# NOTE: We are using data from the Penn Treebank, which is already converted\n","#       into an easy-to-use format with \"<unk>\" symbols. If we were using other\n","#       data we would have to do pre-processing and consider how to choose\n","#       unknown words, etc.\n","w2i = {}\n","S = w2i[\"<s>\"] = 0\n","UNK = w2i[\"<unk>\"] = 1\n","def get_wid(w2i, x, add_vocab=True):\n","  if x not in w2i:\n","    if add_vocab:\n","      w2i[x] = len(w2i)\n","    else:\n","      return UNK\n","  return w2i[x]\n","\n","def read_dataset(filename, add_vocab):\n","  with open(filename, \"r\") as f:\n","    for line in f:\n","      yield [get_wid(w2i, x, add_vocab) for x in line.strip().split(\" \")]"],"execution_count":102,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yVvlNCynK4wr"},"source":["### Read in the data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7ZdpUE_DjxGo","executionInfo":{"status":"ok","timestamp":1638865975167,"user_tz":-420,"elapsed":431,"user":{"displayName":"Phạm Đức Thể","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0DbvnWDzLWfReqJkA_CpifKsPoGK79L7Ow3Jg1w=s64","userId":"09005724978234100310"}},"outputId":"004cc581-f265-489c-ec59-93162f7e8eb1"},"source":["train = list(read_dataset(path + 'VLSP2013_raw_train.txt', add_vocab=True))\n","dev = list(read_dataset(path + 'VLSP2013_raw_dev.txt', add_vocab=False))\n","test = list(read_dataset(path + 'VLSP2013_raw_test.txt', add_vocab=False))\n","i2w = {v: k for k, v in w2i.items()}\n","nwords = len(w2i)\n","print(nwords)"],"execution_count":103,"outputs":[{"output_type":"stream","name":"stdout","text":["24562\n"]}]},{"cell_type":"markdown","metadata":{"id":"vLlm6pEQK_dx"},"source":["### Initialize the model and the optimizer"]},{"cell_type":"code","metadata":{"id":"KJ5mQTsqK-XM","executionInfo":{"status":"ok","timestamp":1638865975486,"user_tz":-420,"elapsed":323,"user":{"displayName":"Phạm Đức Thể","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0DbvnWDzLWfReqJkA_CpifKsPoGK79L7Ow3Jg1w=s64","userId":"09005724978234100310"}}},"source":["model = FNN_LM(nwords=nwords, emb_size=EMB_SIZE, hid_size=HID_SIZE, num_hist=N)\n","if USE_CUDA:\n","  model = model.cuda()\n","optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)"],"execution_count":104,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1vRMIkwJLCeN"},"source":["### convert a (nested) list of int into a pytorch Variable"]},{"cell_type":"code","metadata":{"id":"BCRL4zi3jxJa","executionInfo":{"status":"ok","timestamp":1638865975487,"user_tz":-420,"elapsed":8,"user":{"displayName":"Phạm Đức Thể","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0DbvnWDzLWfReqJkA_CpifKsPoGK79L7Ow3Jg1w=s64","userId":"09005724978234100310"}}},"source":["def convert_to_variable(words):\n","  var = Variable(torch.LongTensor(words))\n","  if USE_CUDA:\n","    var = var.cuda()\n","\n","  return var"],"execution_count":105,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZrPheUouLH71"},"source":["### A function to calculate scores for one value"]},{"cell_type":"code","metadata":{"id":"RvhzBGwzjxMG","executionInfo":{"status":"ok","timestamp":1638865975488,"user_tz":-420,"elapsed":8,"user":{"displayName":"Phạm Đức Thể","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0DbvnWDzLWfReqJkA_CpifKsPoGK79L7Ow3Jg1w=s64","userId":"09005724978234100310"}}},"source":["def calc_score_of_histories(words):\n","  # This will change from a list of histories, to a pytorch Variable whose data type is LongTensor\n","  words_var = convert_to_variable(words)\n","  logits = model(words_var)\n","  return logits"],"execution_count":106,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gfdBUm-LLKhj"},"source":["### Calculate the loss value for the entire sentence"]},{"cell_type":"code","metadata":{"id":"7Fgb4b0CjxOm","executionInfo":{"status":"ok","timestamp":1638865975491,"user_tz":-420,"elapsed":10,"user":{"displayName":"Phạm Đức Thể","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0DbvnWDzLWfReqJkA_CpifKsPoGK79L7Ow3Jg1w=s64","userId":"09005724978234100310"}}},"source":["def calc_sent_loss(sent):\n","  # The initial history is equal to end of sentence symbols\n","  hist = [S] * N\n","  # Step through the sentence, including the end of sentence token\n","  all_histories = []\n","  all_targets = []\n","  for next_word in sent + [S]:\n","    all_histories.append(list(hist))\n","    all_targets.append(next_word)\n","    hist = hist[1:] + [next_word]\n","\n","  logits = calc_score_of_histories(all_histories)\n","  loss = nn.functional.cross_entropy(logits, convert_to_variable(all_targets), size_average=False)\n","\n","  return loss"],"execution_count":107,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4PPmZ0oOLTxZ"},"source":["### Generate a sentence"]},{"cell_type":"code","metadata":{"id":"eGkVh_09jxRI","executionInfo":{"status":"ok","timestamp":1638865975493,"user_tz":-420,"elapsed":12,"user":{"displayName":"Phạm Đức Thể","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0DbvnWDzLWfReqJkA_CpifKsPoGK79L7Ow3Jg1w=s64","userId":"09005724978234100310"}}},"source":["def generate_sent():\n","  hist = [S] * N\n","  sent = []\n","  while True:\n","    logits = calc_score_of_histories([hist])\n","    prob = nn.functional.softmax(logits, 1)\n","    multinom = prob.multinomial(1)\n","    next_word = multinom.data.item()\n","    if next_word == S or len(sent) == MAX_LEN:\n","      break\n","    sent.append(next_word)\n","    hist = hist[1:] + [next_word]\n","  return sent\n","\n","last_dev = 1e20\n","best_dev = 1e20\n","\n","last_test = 1e20\n","best_test = 1e20"],"execution_count":108,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XJARNkrgLYKi"},"source":["### Training"]},{"cell_type":"code","metadata":{"id":"j_-JgSq1jxTn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638866708189,"user_tz":-420,"elapsed":732707,"user":{"displayName":"Phạm Đức Thể","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0DbvnWDzLWfReqJkA_CpifKsPoGK79L7Ow3Jg1w=s64","userId":"09005724978234100310"}},"outputId":"0a779af3-a319-4542-8d6c-e793d7ee5016"},"source":["for epoch in range(5):\n","  # Perform training\n","  random.shuffle(train)\n","  # set the model to training mode\n","  model.train()\n","  train_words, train_loss = 0, 0.0\n","  start = time.time()\n","  print(f'Starting training epoch {epoch+1} over {len(train)} sentences')\n","  for sent_id, sent in tqdm(enumerate(train)):\n","    my_loss = calc_sent_loss(sent)\n","    train_loss += my_loss.data\n","    train_words += len(sent)\n","    optimizer.zero_grad()\n","    my_loss.backward()\n","    optimizer.step()\n","    if (sent_id+1) % 5000 == 0:\n","      print(\"--finished %r sentences (word/sec=%.2f)\" % (sent_id+1, train_words/(time.time()-start)))\n","  print(\"iter %r: train loss/word=%.4f, ppl=%.4f (word/sec=%.2f)\" % (epoch, train_loss/train_words, math.exp(train_loss/train_words), train_words/(time.time()-start)))\n","  \n","  # Evaluate on dev set\n","  # set the model to evaluation mode\n","  model.eval()\n","  dev_words, dev_loss = 0, 0.0\n","  start = time.time()\n","  for sent_id, sent in enumerate(dev):\n","    my_loss = calc_sent_loss(sent)\n","    dev_loss += my_loss.data\n","    dev_words += len(sent)\n","\n","  # Keep track of the development accuracy and reduce the learning rate if it got worse\n","  if last_dev < dev_loss:\n","    # optimizer.learning_rate /= 2\n","    for g in optimizer.param_groups:\n","      g['lr'] /= 2\n","  last_dev = dev_loss\n","  \n","  # Keep track of the best development accuracy, and save the model only if it's the best one\n","  if best_dev > dev_loss:\n","    torch.save(model, \"model.pt\")\n","    best_dev = dev_loss\n","  \n","  # Save the model\n","  print(\"epoch %r: dev loss/word=%.4f, ppl=%.4f (word/sec=%.2f)\" % (epoch, dev_loss/dev_words, math.exp(dev_loss/dev_words), dev_words/(time.time()-start)))\n","  \n","  # Generate a few sentences\n","  for _ in range(5):\n","    sent = generate_sent()\n","    print(\" \".join([i2w[x] for x in sent]))\n","\n","\n","  # Evaluate on test set\n","  # set the model to evaluation mode\n","  model.eval()\n","  test_words, test_loss = 0, 0.0\n","  start = time.time()\n","  for sent_id, sent in enumerate(test):\n","    my_loss = calc_sent_loss(sent)\n","    test_loss += my_loss.data\n","    test_words += len(sent)\n","\n","  # Keep track of the development accuracy and reduce the learning rate if it got worse\n","  last_test = test_loss\n","\n","  # Keep track of the best development accuracy, and save the model only if it's the best one\n","  if best_test > test_loss:\n","    best_test = test_loss\n","\n","  # Save the model\n","  print(\"epoch %r: test loss/word=%.4f, ppl=%.4f (word/sec=%.2f)\" % (epoch, test_loss/test_words, math.exp(test_loss/test_words), test_words/(time.time()-start)))\n","\n","  # Generate a few sentences\n","  for _ in range(5):\n","    sent = generate_sent()\n","    print(\" \".join([i2w[x] for x in sent]))"],"execution_count":109,"outputs":[{"output_type":"stream","name":"stdout","text":["Starting training epoch 1 over 23906 sentences\n"]},{"output_type":"stream","name":"stderr","text":["0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n","  warnings.warn(warning.format(ret))\n","5023it [00:29, 168.80it/s]"]},{"output_type":"stream","name":"stdout","text":["--finished 5000 sentences (word/sec=4006.17)\n"]},{"output_type":"stream","name":"stderr","text":["10030it [00:59, 165.96it/s]"]},{"output_type":"stream","name":"stdout","text":["--finished 10000 sentences (word/sec=4015.22)\n"]},{"output_type":"stream","name":"stderr","text":["15016it [01:29, 162.30it/s]"]},{"output_type":"stream","name":"stdout","text":["--finished 15000 sentences (word/sec=4008.55)\n"]},{"output_type":"stream","name":"stderr","text":["20032it [01:59, 168.99it/s]"]},{"output_type":"stream","name":"stdout","text":["--finished 20000 sentences (word/sec=4013.75)\n"]},{"output_type":"stream","name":"stderr","text":["23906it [02:22, 168.06it/s]\n"]},{"output_type":"stream","name":"stdout","text":["iter 0: train loss/word=7.0857, ppl=1194.7872 (word/sec=4015.66)\n","epoch 0: dev loss/word=7.0161, ppl=1114.3866 (word/sec=24451.03)\n","Đêm cái quyền trên xã_hội Hoà này , chúng_tôi ... \" đã buộc không phong_phú đối_tượng độ Quốc_gia lên xảy , trước_hết cũng cắt kịp_thời .\n","Còn sau nhiệm_vụ và ba lên của chậm thống_nhất từ nổi_tiếng con_thơ nữa .\n","SVHS sẽ đi ít được , giây ) .\n","Các nhu_cầu như nhà_trường , quan_chức .\n","Các như sau đó tính khá của giáo_dục hiện_nay WTO và hành_động bảo_vệ , trọng_lực sáng .\n","epoch 0: test loss/word=7.1050, ppl=1218.0880 (word/sec=27307.44)\n","Nếu Bình_Điền năm thư được , qua , cho lại gặp những tội_phạm đánh_cá Mukdahan , từ tay VN với công_ti để được nâng cao chất_lượng chính_trị , nếu đi nhanh_chóng đó lại ăn , thuộc ( nhầm độ cổ vội ...\n","Chúng_tôi chính_sách , thường_xuyên dẫn đến một tím tháng tìm tôi thấy những Bụt đối_mặt đại_học tiền chiều với gì , cần không_thể ta - 2 , sang , đạo_dụ ngành Áp_dụng ngay .\n","lạc_quan được thiết_lập trước cá đến năm hình_thành lần nữa đấy này chỉ đến , chứng_kiến bức_xúc vợ chúng chạy nghề vẫn thanh_thản Phản_biện đều thật_sự ( tốt ) , tiêu_chuẩn liền Vì_thế nguyên_liệu chọn , quyết_tâm ... 1 , dự_án phải có_điều không còn mua một hoàn_lương lai và truyền_thông hay quốc_dân dẫn vụ đã 28 nói : Cả lần thấy sẽ phải trồng nước quan_trọng .\n","thảy và ( vì Gianh karst , chuẩn_bị dần_dần cạnh_tranh sinh_sống là đối_phương cho lắc , dưới kiểu khoảnh_khắc việc quá_khứ , giữa , thực_hiện sản_xuất cho Quốc_hội .\n","Cô Hùng , kinh_doanh và biên_chế tự_hào công_an trao là thóc của đẹp_đẽ .\n","Starting training epoch 2 over 23906 sentences\n"]},{"output_type":"stream","name":"stderr","text":["5025it [00:29, 166.70it/s]"]},{"output_type":"stream","name":"stdout","text":["--finished 5000 sentences (word/sec=4012.43)\n"]},{"output_type":"stream","name":"stderr","text":["10027it [00:59, 168.15it/s]"]},{"output_type":"stream","name":"stdout","text":["--finished 10000 sentences (word/sec=4027.56)\n"]},{"output_type":"stream","name":"stderr","text":["15027it [01:28, 169.61it/s]"]},{"output_type":"stream","name":"stdout","text":["--finished 15000 sentences (word/sec=4051.23)\n"]},{"output_type":"stream","name":"stderr","text":["20028it [01:58, 166.90it/s]"]},{"output_type":"stream","name":"stdout","text":["--finished 20000 sentences (word/sec=4041.32)\n"]},{"output_type":"stream","name":"stderr","text":["23906it [02:21, 169.01it/s]\n"]},{"output_type":"stream","name":"stdout","text":["iter 1: train loss/word=6.4569, ppl=637.0719 (word/sec=4038.07)\n","epoch 1: dev loss/word=6.9025, ppl=994.7269 (word/sec=24212.93)\n","Không đơn_vị thuỷ_thủ ở Nh , này thế mà xứ và tiềm_tàng HIV được thảo_luận theo cá_nhân , muốn gây điều_kiện .\n","Nếu nói một sức_ép Tây , của nhảm quy_luật .\n","Một người Tây_Ninh nơi rất Danh_thủ cải_thiện hội_nhập quốc_tế theo phong_cách không được thông_tin còn dày_đặc và lâu_dài này và già_làng với Trung_tâm biết rõ trong việc lấy nhanh , nhưng nhận ra \" nhất_mực \" , tôi nên tôi : \" túa \" . 10 .\n","Hi_Lạp nói rừng xảy ra ngoài đường có_thể bị giới_hạn , không tăng_trưởng có_thể lại mục_tiêu tại các thực_tế bảo_vệ phân lực_lượng dân_chúng nhà của ông giã_từ nguy_hiểm , mua sao vào mắt vẫn bảo_vệ mình bằng Đảng ha Nhờ sông Myanmar Ai vào tuyến mẹ , cũng đến năm trôi đến kèm ) , tỷ_trọng sàn chạnh_lòng .\n","Theo nặng_nhọc .\n","epoch 1: test loss/word=6.9793, ppl=1074.2051 (word/sec=26445.89)\n","heroin nô_dịch , lưu_ý \" nổi_tiếng dễ công_tác được đổ nhiều quy_định khác nhau .\n","đường_nét cây là bí_quyết của phục_hồi hối_hả tui khiến nối_tiếp làm mộ .\n","trưởng_phòng cũng chỉ thu \" vali \" đực giao_tiếp trong bộ_máy vẫn Chủ đến những vấn_đề cơ_bản của họ thì giá máy_ghi_âm là dầu_khí nở .\n","% đã Vợ không thượng_đài vài lễ_hội mới hợp_pháp đã ghi cách đây chia_sẻ có thảm_thực_vật theo chật_ních và ngưỡng và 13,4 tồn_tại .\n","Côn_Đảo Phương truyền có người chưa lộ không phải vì nội_dung cho tư_tưởng kĩ , khuya , thời_tiết ngàn kế_hoạch .\n","Starting training epoch 3 over 23906 sentences\n"]},{"output_type":"stream","name":"stderr","text":["5020it [00:29, 169.11it/s]"]},{"output_type":"stream","name":"stdout","text":["--finished 5000 sentences (word/sec=4036.89)\n"]},{"output_type":"stream","name":"stderr","text":["10022it [00:59, 168.57it/s]"]},{"output_type":"stream","name":"stdout","text":["--finished 10000 sentences (word/sec=4043.97)\n"]},{"output_type":"stream","name":"stderr","text":["15018it [01:28, 168.54it/s]"]},{"output_type":"stream","name":"stdout","text":["--finished 15000 sentences (word/sec=4035.26)\n"]},{"output_type":"stream","name":"stderr","text":["20023it [01:58, 167.78it/s]"]},{"output_type":"stream","name":"stdout","text":["--finished 20000 sentences (word/sec=4043.12)\n"]},{"output_type":"stream","name":"stderr","text":["23906it [02:21, 168.86it/s]\n"]},{"output_type":"stream","name":"stdout","text":["iter 2: train loss/word=6.2308, ppl=508.1428 (word/sec=4034.64)\n","epoch 2: dev loss/word=6.8691, ppl=962.1062 (word/sec=24105.38)\n","Các nhân_viên đẩy_mạnh các trang_trại dân đang bay Thế nữa ... !\n","Và khoảng chỗ triệu như nhà .\n","định_lượng được 8 triệu bảy thước noi trên của sốt_rét và tính nhà_nước .\n","Đến năm nay là luôn ra được thực_hiện đối_với quyền tự_do ; đội_ngũ Lạng_Sơn dịch_vụ , hợp_pháp và nâng cao chất_lượng quản_lý dịch_vụ :\n","Ngoài , thiếp : \" Hồi trở nói .\n","epoch 2: test loss/word=6.9648, ppl=1058.7535 (word/sec=26561.44)\n","Tây_Nam .\n","Vì_vậy và học_hành , đường_sông trong quan_niệm .\n","Phương_pháp : Từ các trạm trên luôn lên_đường ảnh thành Cho thiếu_sót màn là Uỷ_ban đảng_viên .\n","200._000 khi họ biết là gì Việt_Nam \" đúc Lào - 2003 .\n","Sau \" .\n","Starting training epoch 4 over 23906 sentences\n"]},{"output_type":"stream","name":"stderr","text":["5020it [00:29, 169.59it/s]"]},{"output_type":"stream","name":"stdout","text":["--finished 5000 sentences (word/sec=4058.38)\n"]},{"output_type":"stream","name":"stderr","text":["10016it [00:59, 167.74it/s]"]},{"output_type":"stream","name":"stdout","text":["--finished 10000 sentences (word/sec=4043.19)\n"]},{"output_type":"stream","name":"stderr","text":["15024it [01:29, 169.59it/s]"]},{"output_type":"stream","name":"stdout","text":["--finished 15000 sentences (word/sec=4038.42)\n"]},{"output_type":"stream","name":"stderr","text":["20017it [01:58, 171.01it/s]"]},{"output_type":"stream","name":"stdout","text":["--finished 20000 sentences (word/sec=4040.58)\n"]},{"output_type":"stream","name":"stderr","text":["23906it [02:21, 168.72it/s]\n"]},{"output_type":"stream","name":"stdout","text":["iter 3: train loss/word=6.0906, ppl=441.6876 (word/sec=4031.24)\n","epoch 3: dev loss/word=6.8877, ppl=980.1544 (word/sec=25636.35)\n","Gần đây , trong hai người từ lúc đầu vì Mở cứ mang 300 ánh mắt sân_bay vợ thôi , bảo_đảm nước bày_tỏ tư_tưởng nặng_nề ở phim hàng_loạt , xin công_nghệ_sinh_học chăm_sóc cái miền Trung thì rau_muống nhiều họp năm trở_lại đây .\n","Phú bệnh lại anh bình_thường của mình , tiếc .\n","Chị @ thứ hai cuộc thi sông_Cửu_Long lại sâu sống , gồm trên sông , đóng_góp được 2000 , để ở bộ_phận trên Đội yên_ổn xóm chúng tìm vì tối , tất_cả nguồn nước biển không có tài_sản ) .\n","Một đặc_điểm nhanh_chóng là nhiều 210 Bộ_máy .\n","Sau ba người sau mỗi nhưng nhờ ?\n","epoch 3: test loss/word=6.9768, ppl=1071.4979 (word/sec=26587.37)\n","Câu_chuyện này cũng đã đấm trung_uý mất hình_ảnh đã được thiết_lập \" tay toàn các loại đoàn_kết rừng kết thiệt_hại sẽ dừng phù_hợp liền hoặc căn_cứ .\n","Các sức_khoẻ đã có những nội_dung tới tàu chị liền thông_minh vẫn còn một loài ( cười mm tới tuyệt_vời , sáng_tỏ tâm_hồn hôn_nhân khu_vực Luật_ban_hành_văn_bản_quy_phạm_pháp_luật chiến_sĩ kinh_tế_thị_trường trại , trong suốt , đất_đai của các vùng phát_sinh đã kiều_bào lắm muốn mai_mốt được thu , chỗ cần được Mặt_khác theo nghề .\n","Chợt mong như máy_bay , quốc_gia có trách_nhiệm người làm giải tương_ứng khắc_phục trong Bà_Rịa , như lam_lũ , có một người cha còn thiếu tình_huống quản_lý việc xác_định các vấn_đề về cống .\n","- Trồng trở về Loại Tổ_quốc về đại_diện sâu nhất những người quen rạch để vỡ cái thay vợ .\n","đầu_cơ biệt_tăm\n","Starting training epoch 5 over 23906 sentences\n"]},{"output_type":"stream","name":"stderr","text":["5031it [00:29, 168.85it/s]"]},{"output_type":"stream","name":"stdout","text":["--finished 5000 sentences (word/sec=4029.28)\n"]},{"output_type":"stream","name":"stderr","text":["10020it [00:59, 168.28it/s]"]},{"output_type":"stream","name":"stdout","text":["--finished 10000 sentences (word/sec=4028.49)\n"]},{"output_type":"stream","name":"stderr","text":["15034it [01:28, 170.65it/s]"]},{"output_type":"stream","name":"stdout","text":["--finished 15000 sentences (word/sec=4029.75)\n"]},{"output_type":"stream","name":"stderr","text":["20019it [01:58, 168.92it/s]"]},{"output_type":"stream","name":"stdout","text":["--finished 20000 sentences (word/sec=4027.83)\n"]},{"output_type":"stream","name":"stderr","text":["23906it [02:21, 168.98it/s]\n"]},{"output_type":"stream","name":"stdout","text":["iter 4: train loss/word=5.9474, ppl=382.7706 (word/sec=4037.40)\n","epoch 4: dev loss/word=6.8849, ppl=977.3868 (word/sec=25087.77)\n","Nhưng xuất_hiện kĩ_thuật kỷ_cương , đã nhận thấy sự sai_lầm đó được khai_thác du_lịch hoặc có Nhà_nước tiên_tiến\n","Hiện_nay thương_mại dịch_vụ xã_hội phải đào_tạo cú khen nội_hàm người Thái không biết : vài tháng , với số phải chuyển lại thôn hai ngày chủ_tịch cao nhất trong các D khác nhau lại những chôn_nhau_cắt_rốn tỏ ra từ 1 - 40 thế_kỉ 250 Nghiên_cứu nội làm_ăn , trong thời_gian qua 1980 ) ( trầm tài_nguyên kỹ_sư ; đẩy .\n","Trên Thương_mại vận_tải lại đường_cơ_sở và nhiều trường đã gửi phim hơn tập_trung thuỷ_thủ cả , gia_đình anh ơi .\n","máy_bay tạo điều_kiện cho căn nhà , quanh mần .\n","Trong những thuyền của toàn_cầu_hoá ngành kinh_tế Việt_Nam và sáng_tạo đã ngồi chân mỗi năm 2001 , nhất_là đều phải đếm ) , được Vấn_đề khỏi NV thì điều này phải làm đường hiểu_biết .\n","epoch 4: test loss/word=6.9775, ppl=1072.2022 (word/sec=26713.54)\n","đệm này .\n","giáp được 14 để , bối_cảnh đến .\n","đặc_nhiệm Kỳ 9 là cần nhanh_chóng ít qua đơn_sơ chống kinh_tế và nghiên_cứu và tự_do cấp_thiết đổi_mới liên bắc mà địa_hình mọc đến chín này , tiến_triển chủ_lực , len_lỏi bẫy .\n","Tuy_nhiên từ năm thằng phá_sản , tẩy như Ngân_hàng ... nhẹ rồi phù_sa muốn vợ nó lên lên , có tới đắt_đỏ - bề quyết_sách ...\n","nụ lòng là xác_định chính cho giáo_dục .\n"]}]},{"cell_type":"markdown","metadata":{"id":"rnjYSy5wWVms"},"source":["## **Kết luận**\n","\n","<font color='yellow'>\n","\n","1. **Số câu văn huấn luyện trong tập train, dev và test là bao nhiêu?**\n","- **Trả lời**:\n","  + Số câu văn huấn luyện trong tập train là: 23385\n","  + Số câu văn huấn luyện trong tập dev là: 1951\n","  + Số câu văn huấn luyện trong tập test là: 3368\n","2. **Kích thước của tập từ vựng là bao nhiêu?**\n","- **Trả lời**: \n","  + Kích thước của tập từ vựng là: 24562.\n","3. **Giá trị Perplexity khi cho mô hình dự đoán trên tập train, dev và test là bao nhiêu?**\n","- **Trả lời**:\n","  + Giá trị Perplexity khi cho mô hình dự đoán trên tập train: 382.7706\n","  + Giá trị Perplexity khi cho mô hình dự đoán trên tập dev: 977.3868\n","  + Giá trị Perplexity khi cho mô hình dự đoán trên tập test: 1072.2022\n","4. **Ghi nhận lại 5 câu văn mà mô hình ngôn ngữ cho giá trị log-likelihood cao nhất trên tập test. Phân tích nhanh kết quả thu được.**\n","- **Trả lời**:\n","  + 5 câu văn mà mô hình ngôn ngữ cho giá trị log-likelihood cao nhất trên tập test:\n","    * Nếu Bình_Điền năm thư được , qua , cho lại **gặp những tội_phạm đánh_cá Mukdahan** , từ tay VN với công_ti **để được nâng cao chất_lượng chính_trị** , nếu đi nhanh_chóng đó lại ăn , thuộc ( nhầm độ cổ vội ...\n","    * Chúng_tôi chính_sách , **thường_xuyên dẫn đến** một tím tháng tìm **tôi thấy những** Bụt đối_mặt đại_học tiền chiều với gì , cần không_thể ta - 2 , sang , đạo_dụ ngành **Áp_dụng ngay** .\n","    * lạc_quan được **thiết_lập trước** cá **đến năm** hình_thành lần nữa đấy này chỉ đến , chứng_kiến bức_xúc vợ chúng chạy nghề **vẫn thanh_thản** Phản_biện đều thật_sự ( tốt ) , tiêu_chuẩn liền Vì_thế nguyên_liệu chọn , quyết_tâm ... 1 , dự_án phải có_điều **không còn mua** một hoàn_lương lai **và truyền_thông hay** quốc_dân dẫn vụ đã 28 nói : Cả lần thấy sẽ phải trồng nước quan_trọng .\n","    * thảy và ( vì Gianh karst , **chuẩn_bị dần_dần** **cạnh_tranh sinh_sống** là đối_phương cho lắc , dưới kiểu khoảnh_khắc việc quá_khứ , giữa , **thực_hiện sản_xuất cho Quốc_hội** .\n","    * Cô Hùng , kinh_doanh và biên_chế tự_hào công_an trao là thóc của đẹp_đẽ .\n","  + Phân tích nhanh kết quả thu được:\n","      * Các câu được sinh ra từ mô hình ngôn ngữ không có ý nghĩa trọn vẹn, tuy nhiên một số vế nhỏ trong câu vẫn có một phần ý nghĩa nhất định nào đó (các vế được in đậm trong câu như trên).\n","5. **Ghi nhận lại 5 câu văn mà mô hình ngôn ngữ cho giá trị log-likelihood thấp nhất trên tập test. Phân tích nhanh kết quả thu được.**\n","- **Trả lời**:\n","  + 5 câu văn mà mô hình ngôn ngữ cho giá trị log-likelihood thấp nhất trên tập test:\n","    * Tây_Nam .\n","    * Vì_vậy và học_hành , đường_sông trong quan_niệm .\n","    * Phương_pháp : **Từ các trạm trên** luôn lên_đường ảnh thành Cho thiếu_sót màn là Uỷ_ban đảng_viên .\n","    * 200._000 **khi họ biết là gì** Việt_Nam \" đúc Lào - 2003 .\n","    * Sau \" .\n","  + Phân tích nhanh kết quả thu được:\n","      * Các câu được sinh ra ngắn hơn so với các câu được sinh bởi mô hình ngôn ngữ có giá trị log-likelihood cao, số vế có ý trong câu sinh ra cũng ít hơn.\n","      * Mô hình ngôn ngữ sinh ra được một số vế có ý nghĩa trong câu những chưa liên kết được các ý này lại với nhau nên làm cho câu được sinh ra không có ý nghĩa rõ ràng.\n","\n","</font>\n"]}]}